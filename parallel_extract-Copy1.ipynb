{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspired by notebook from Aguiar\n",
    "# Func: Extract all features, flatten json columns\n",
    "# with multiprocessing module\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import gc\n",
    "import sys\n",
    "import math\n",
    "from pandas.io.json import json_normalize\n",
    "from datetime import datetime\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from ast import literal_eval\n",
    "import multiprocessing\n",
    "import glob\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataframe processing module\n",
    "JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n",
    "def work_on_one_reader(df,index,flag):\n",
    "    df.reset_index(drop = True,inplace = True)\n",
    "    for column in JSON_COLUMNS:\n",
    "        column_as_df = json_normalize(df[column])\n",
    "        column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns]\n",
    "        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n",
    "\n",
    "        # Normalize customDimensions\n",
    "    df['customDimensions']=df['customDimensions'].apply(literal_eval)\n",
    "    df['customDimensions']=df['customDimensions'].str[0]\n",
    "    df['customDimensions']=df['customDimensions'].apply(lambda x: {'index':np.NaN,'value':np.NaN} if pd.isnull(x) else x)\n",
    "\n",
    "    column_as_df = json_normalize(df['customDimensions'])\n",
    "    column_as_df.columns = [f\"customDimensions_{subcolumn}\" for subcolumn in column_as_df.columns]\n",
    "    df = df.drop('customDimensions', axis=1).merge(column_as_df, right_index=True, left_index=True)\n",
    "\n",
    "    # ===============================================\n",
    "#     print(\"---> working on hits\")\n",
    "    feat = 'hits'\n",
    "    df[feat]=df[feat].apply(literal_eval)\n",
    "    df[feat]=df[feat].str[0]\n",
    "    df[feat]=df[feat].apply(lambda x: {'index':np.NaN} if pd.isnull(x) else x)\n",
    "    column_as_df = json_normalize(df[feat])\n",
    "    column_as_df.columns = [f\"hits_{subcolumn}\" for subcolumn in column_as_df.columns]\n",
    "    df = df.drop(feat, axis=1).merge(column_as_df, right_index=True, left_index=True)\n",
    "\n",
    "    # ===============================================\n",
    "#     print(\"---> working on hits_promotion\")\n",
    "    feat = 'hits_promotion'\n",
    "#     df[feat]=df[feat].apply(literal_eval)\n",
    "    df[feat]=df[feat].str[0]\n",
    "    df[feat]=df[feat].apply(lambda x: {'index':np.NaN} if pd.isnull(x) else x)\n",
    "    column_as_df = json_normalize(df[feat])\n",
    "    column_as_df.columns = [f\"hits_promotion_{subcolumn}\" for subcolumn in column_as_df.columns]\n",
    "    df = df.drop(feat, axis=1).merge(column_as_df, right_index=True, left_index=True)\n",
    "\n",
    "\n",
    "    # ===============================================\n",
    "#     print(\"---> working on hits_product\")\n",
    "    feat = 'hits_product'\n",
    "#     df[feat]=df[feat].apply(literal_eval)\n",
    "    df[feat]=df[feat].str[0]\n",
    "    df[feat]=df[feat].apply(lambda x: {'index':np.NaN} if pd.isnull(x) else x)\n",
    "    column_as_df = json_normalize(df[feat])\n",
    "    column_as_df.columns = [f\"hits_product_{subcolumn}\" for subcolumn in column_as_df.columns]\n",
    "    df = df.drop(feat, axis=1).merge(column_as_df, right_index=True, left_index=True)\n",
    "\n",
    "    bracket_col = ['hits_customDimensions','hits_customMetrics','hits_customVariables','hits_experiment',\n",
    "               'hits_publisher_infos','hits_product_customDimensions','hits_product_customMetrics']\n",
    "    for col in bracket_col:\n",
    "        df[col] = df[col].str[0]\n",
    "    out_name = \"./data/out.{0}.{1}.csv\".format(flag,index)\n",
    "    df.to_csv(out_name,index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_input_csv(csv_path,chunksize=5000,flag=None):\n",
    "    time_beg = datetime.now()\n",
    "    dfs = pd.read_csv(csv_path, sep=',',\n",
    "            converters={column: json.loads for column in JSON_COLUMNS}, \n",
    "            dtype={'fullVisitorId': 'str'}, # Important!!\n",
    "            chunksize=chunksize)\n",
    "    jobs = []\n",
    "    for index,df in enumerate(dfs):\n",
    "        print(\"--> job {} started\".format(index))\n",
    "        p = multiprocessing.Process(target=work_on_one_reader,args=(df,index,flag,))\n",
    "        p.start()\n",
    "        jobs.append(p)\n",
    "    for index,job in enumerate(jobs):\n",
    "        job.join()\n",
    "    \n",
    "    # read in all processed csv file and concat together\n",
    "    path = \"./data\"\n",
    "    all_file = glob.glob(path+\"/out.{}.*.csv\".format(flag))\n",
    "    list_ = []\n",
    "    new_df = pd.DataFrame()\n",
    "    for file in all_file:\n",
    "        df = pd.read_csv(file,low_memory=False)\n",
    "        list_.append(df)\n",
    "    new_df = pd.concat(list_)\n",
    "    time_end = datetime.now()\n",
    "    print(\"----> Total time spent is {}\".format(time_end - time_beg))\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> job 0 started\n",
      "--> job 1 started\n",
      "--> job 2 started\n",
      "--> job 3 started\n",
      "--> job 4 started\n",
      "--> job 5 started\n",
      "--> job 6 started\n",
      "--> job 7 started\n",
      "--> job 8 started\n",
      "--> job 9 started\n",
      "--> job 10 started\n",
      "--> job 11 started\n",
      "--> job 12 started\n",
      "--> job 13 started\n",
      "--> job 14 started\n",
      "--> job 15 started\n",
      "--> job 16 started\n",
      "--> job 17 started\n",
      "--> job 18 started\n",
      "--> job 19 started\n",
      "--> job 20 started\n",
      "--> job 21 started\n",
      "--> job 22 started\n",
      "--> job 23 started\n",
      "--> job 24 started\n",
      "--> job 25 started\n",
      "--> job 26 started\n",
      "--> job 27 started\n",
      "--> job 28 started\n",
      "--> job 29 started\n",
      "--> job 30 started\n",
      "--> job 31 started\n",
      "--> job 32 started\n",
      "--> job 33 started\n",
      "--> job 34 started\n",
      "--> job 35 started\n",
      "--> job 36 started\n",
      "--> job 37 started\n",
      "--> job 38 started\n",
      "--> job 39 started\n",
      "--> job 40 started\n",
      "--> job 41 started\n",
      "--> job 42 started\n",
      "--> job 43 started\n",
      "--> job 44 started\n",
      "--> job 45 started\n",
      "--> job 46 started\n",
      "--> job 47 started\n",
      "--> job 48 started\n",
      "--> job 49 started\n",
      "--> job 50 started\n",
      "--> job 51 started\n",
      "--> job 52 started\n",
      "--> job 53 started\n",
      "--> job 54 started\n",
      "--> job 55 started\n",
      "--> job 56 started\n",
      "--> job 57 started\n",
      "--> job 58 started\n",
      "--> job 59 started\n",
      "--> job 60 started\n",
      "--> job 61 started\n",
      "--> job 62 started\n",
      "--> job 63 started\n",
      "--> job 64 started\n",
      "--> job 65 started\n",
      "--> job 66 started\n",
      "--> job 67 started\n",
      "--> job 68 started\n",
      "--> job 69 started\n",
      "--> job 70 started\n",
      "--> job 71 started\n",
      "--> job 72 started\n",
      "--> job 73 started\n",
      "--> job 74 started\n",
      "--> job 75 started\n",
      "--> job 76 started\n",
      "--> job 77 started\n",
      "--> job 78 started\n",
      "--> job 79 started\n",
      "--> job 80 started\n",
      "--> job 81 started\n",
      "--> job 82 started\n",
      "--> job 83 started\n",
      "--> job 84 started\n",
      "--> job 85 started\n",
      "--> job 86 started\n",
      "--> job 87 started\n",
      "--> job 88 started\n",
      "--> job 89 started\n",
      "--> job 90 started\n",
      "--> job 91 started\n",
      "--> job 92 started\n",
      "--> job 93 started\n",
      "--> job 94 started\n",
      "--> job 95 started\n",
      "--> job 96 started\n",
      "--> job 97 started\n",
      "--> job 98 started\n",
      "--> job 99 started\n",
      "--> job 100 started\n",
      "--> job 101 started\n",
      "--> job 102 started\n",
      "--> job 103 started\n",
      "--> job 104 started\n",
      "--> job 105 started\n",
      "--> job 106 started\n",
      "--> job 107 started\n",
      "--> job 108 started\n",
      "--> job 109 started\n",
      "--> job 110 started\n",
      "--> job 111 started\n",
      "--> job 112 started\n",
      "--> job 113 started\n",
      "--> job 114 started\n",
      "--> job 115 started\n",
      "--> job 116 started\n",
      "--> job 117 started\n",
      "--> job 118 started\n",
      "--> job 119 started\n",
      "--> job 120 started\n",
      "--> job 121 started\n",
      "--> job 122 started\n",
      "--> job 123 started\n",
      "--> job 124 started\n",
      "--> job 125 started\n",
      "--> job 126 started\n",
      "--> job 127 started\n",
      "--> job 128 started\n",
      "--> job 129 started\n",
      "--> job 130 started\n",
      "--> job 131 started\n",
      "--> job 132 started\n",
      "--> job 133 started\n",
      "--> job 134 started\n",
      "--> job 135 started\n",
      "--> job 136 started\n",
      "--> job 137 started\n",
      "--> job 138 started\n",
      "--> job 139 started\n",
      "--> job 140 started\n",
      "--> job 141 started\n",
      "--> job 142 started\n",
      "--> job 143 started\n",
      "--> job 144 started\n",
      "--> job 145 started\n",
      "--> job 146 started\n",
      "--> job 147 started\n",
      "--> job 148 started\n",
      "--> job 149 started\n",
      "--> job 150 started\n",
      "--> job 151 started\n",
      "--> job 152 started\n",
      "--> job 153 started\n",
      "--> job 154 started\n",
      "--> job 155 started\n",
      "--> job 156 started\n",
      "--> job 157 started\n",
      "--> job 158 started\n",
      "--> job 159 started\n",
      "--> job 160 started\n",
      "--> job 161 started\n",
      "--> job 162 started\n",
      "--> job 163 started\n",
      "--> job 164 started\n",
      "--> job 165 started\n",
      "--> job 166 started\n",
      "--> job 167 started\n",
      "--> job 168 started\n",
      "--> job 169 started\n",
      "--> job 170 started\n",
      "--> job 171 started\n",
      "--> job 172 started\n",
      "--> job 173 started\n",
      "--> job 174 started\n",
      "--> job 175 started\n",
      "--> job 176 started\n",
      "--> job 177 started\n",
      "--> job 178 started\n",
      "--> job 179 started\n",
      "--> job 180 started\n",
      "--> job 181 started\n",
      "--> job 182 started\n",
      "--> job 183 started\n",
      "--> job 184 started\n",
      "--> job 185 started\n",
      "--> job 186 started\n",
      "--> job 187 started\n",
      "--> job 188 started\n",
      "--> job 189 started\n",
      "--> job 190 started\n",
      "--> job 191 started\n",
      "--> job 192 started\n",
      "--> job 193 started\n",
      "--> job 194 started\n",
      "--> job 195 started\n",
      "--> job 196 started\n",
      "--> job 197 started\n",
      "--> job 198 started\n",
      "--> job 199 started\n",
      "--> job 200 started\n",
      "--> job 201 started\n",
      "--> job 202 started\n",
      "--> job 203 started\n",
      "--> job 204 started\n",
      "--> job 205 started\n",
      "--> job 206 started\n",
      "--> job 207 started\n",
      "--> job 208 started\n",
      "--> job 209 started\n",
      "--> job 210 started\n",
      "--> job 211 started\n",
      "--> job 212 started\n",
      "--> job 213 started\n",
      "--> job 214 started\n",
      "--> job 215 started\n",
      "--> job 216 started\n",
      "--> job 217 started\n",
      "--> job 218 started\n",
      "--> job 219 started\n",
      "--> job 220 started\n",
      "--> job 221 started\n",
      "--> job 222 started\n",
      "--> job 223 started\n",
      "--> job 224 started\n",
      "--> job 225 started\n",
      "--> job 226 started\n",
      "--> job 227 started\n",
      "--> job 228 started\n",
      "--> job 229 started\n",
      "--> job 230 started\n",
      "--> job 231 started\n",
      "--> job 232 started\n",
      "--> job 233 started\n",
      "--> job 234 started\n",
      "--> job 235 started\n",
      "--> job 236 started\n",
      "--> job 237 started\n",
      "--> job 238 started\n",
      "--> job 239 started\n",
      "--> job 240 started\n",
      "--> job 241 started\n",
      "--> job 242 started\n",
      "--> job 243 started\n",
      "--> job 244 started\n",
      "--> job 245 started\n",
      "--> job 246 started\n",
      "--> job 247 started\n",
      "--> job 248 started\n",
      "--> job 249 started\n",
      "--> job 250 started\n",
      "--> job 251 started\n",
      "--> job 252 started\n",
      "--> job 253 started\n",
      "--> job 254 started\n",
      "--> job 255 started\n",
      "--> job 256 started\n",
      "--> job 257 started\n",
      "--> job 258 started\n",
      "--> job 259 started\n",
      "--> job 260 started\n",
      "--> job 261 started\n",
      "--> job 262 started\n",
      "--> job 263 started\n",
      "--> job 264 started\n",
      "--> job 265 started\n",
      "--> job 266 started\n",
      "--> job 267 started\n",
      "--> job 268 started\n",
      "--> job 269 started\n",
      "--> job 270 started\n",
      "--> job 271 started\n",
      "--> job 272 started\n",
      "--> job 273 started\n",
      "--> job 274 started\n",
      "--> job 275 started\n",
      "--> job 276 started\n",
      "--> job 277 started\n",
      "--> job 278 started\n",
      "--> job 279 started\n",
      "--> job 280 started\n",
      "--> job 281 started\n",
      "--> job 282 started\n",
      "--> job 283 started\n",
      "--> job 284 started\n",
      "--> job 285 started\n",
      "--> job 286 started\n",
      "--> job 287 started\n",
      "--> job 288 started\n",
      "--> job 289 started\n",
      "--> job 290 started\n",
      "--> job 291 started\n",
      "--> job 292 started\n",
      "--> job 293 started\n",
      "--> job 294 started\n",
      "--> job 295 started\n",
      "--> job 296 started\n",
      "--> job 297 started\n",
      "--> job 298 started\n",
      "--> job 299 started\n",
      "--> job 300 started\n",
      "--> job 301 started\n",
      "--> job 302 started\n",
      "--> job 303 started\n",
      "--> job 304 started\n",
      "--> job 305 started\n",
      "--> job 306 started\n",
      "--> job 307 started\n",
      "--> job 308 started\n",
      "--> job 309 started\n",
      "--> job 310 started\n",
      "--> job 311 started\n",
      "--> job 312 started\n",
      "--> job 313 started\n",
      "--> job 314 started\n",
      "--> job 315 started\n",
      "--> job 316 started\n",
      "--> job 317 started\n",
      "--> job 318 started\n",
      "--> job 319 started\n",
      "--> job 320 started\n",
      "--> job 321 started\n",
      "--> job 322 started\n",
      "--> job 323 started\n",
      "--> job 324 started\n",
      "--> job 325 started\n",
      "--> job 326 started\n",
      "--> job 327 started\n",
      "--> job 328 started\n",
      "--> job 329 started\n",
      "--> job 330 started\n",
      "--> job 331 started\n",
      "--> job 332 started\n",
      "--> job 333 started\n",
      "--> job 334 started\n",
      "--> job 335 started\n",
      "--> job 336 started\n",
      "--> job 337 started\n",
      "--> job 338 started\n",
      "--> job 339 started\n",
      "--> job 340 started\n",
      "--> job 341 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:24: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----> Total time spent is 0:13:01.441712\n"
     ]
    }
   ],
   "source": [
    "train_df = process_input_csv(\"./data/train_v2.csv\",chunksize=5000,flag=\"train\")\n",
    "train_df.to_csv('./data/my_train.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_anaconda3)",
   "language": "python",
   "name": "conda_anaconda3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
