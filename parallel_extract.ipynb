{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspired by notebook from Aguiar\n",
    "# Func: Extract all features, flatten json columns\n",
    "# with multiprocessing module\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import gc\n",
    "import sys\n",
    "import math\n",
    "from pandas.io.json import json_normalize\n",
    "from datetime import datetime\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from ast import literal_eval\n",
    "import multiprocessing\n",
    "import glob\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataframe processing module\n",
    "JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n",
    "def work_on_one_reader(df,index,flag):\n",
    "    df.reset_index(drop = True,inplace = True)\n",
    "    for column in JSON_COLUMNS:\n",
    "        column_as_df = json_normalize(df[column])\n",
    "        column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns]\n",
    "        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n",
    "\n",
    "        # Normalize customDimensions\n",
    "    df['customDimensions']=df['customDimensions'].apply(literal_eval)\n",
    "    df['customDimensions']=df['customDimensions'].str[0]\n",
    "    df['customDimensions']=df['customDimensions'].apply(lambda x: {'index':np.NaN,'value':np.NaN} if pd.isnull(x) else x)\n",
    "\n",
    "    column_as_df = json_normalize(df['customDimensions'])\n",
    "    column_as_df.columns = [f\"customDimensions_{subcolumn}\" for subcolumn in column_as_df.columns]\n",
    "    df = df.drop('customDimensions', axis=1).merge(column_as_df, right_index=True, left_index=True)\n",
    "\n",
    "    # ===============================================\n",
    "#     print(\"---> working on hits\")\n",
    "    feat = 'hits'\n",
    "    df[feat]=df[feat].apply(literal_eval)\n",
    "    df[feat]=df[feat].str[0]\n",
    "    df[feat]=df[feat].apply(lambda x: {'index':np.NaN} if pd.isnull(x) else x)\n",
    "    column_as_df = json_normalize(df[feat])\n",
    "    column_as_df.columns = [f\"hits_{subcolumn}\" for subcolumn in column_as_df.columns]\n",
    "    df = df.drop(feat, axis=1).merge(column_as_df, right_index=True, left_index=True)\n",
    "\n",
    "    # ===============================================\n",
    "#     print(\"---> working on hits_promotion\")\n",
    "    feat = 'hits_promotion'\n",
    "#     df[feat]=df[feat].apply(literal_eval)\n",
    "    df[feat]=df[feat].str[0]\n",
    "    df[feat]=df[feat].apply(lambda x: {'index':np.NaN} if pd.isnull(x) else x)\n",
    "    column_as_df = json_normalize(df[feat])\n",
    "    column_as_df.columns = [f\"hits_promotion_{subcolumn}\" for subcolumn in column_as_df.columns]\n",
    "    df = df.drop(feat, axis=1).merge(column_as_df, right_index=True, left_index=True)\n",
    "\n",
    "\n",
    "    # ===============================================\n",
    "#     print(\"---> working on hits_product\")\n",
    "    feat = 'hits_product'\n",
    "#     df[feat]=df[feat].apply(literal_eval)\n",
    "    df[feat]=df[feat].str[0]\n",
    "    df[feat]=df[feat].apply(lambda x: {'index':np.NaN} if pd.isnull(x) else x)\n",
    "    column_as_df = json_normalize(df[feat])\n",
    "    column_as_df.columns = [f\"hits_product_{subcolumn}\" for subcolumn in column_as_df.columns]\n",
    "    df = df.drop(feat, axis=1).merge(column_as_df, right_index=True, left_index=True)\n",
    "\n",
    "    bracket_col = ['hits_customDimensions','hits_customMetrics','hits_customVariables','hits_experiment',\n",
    "               'hits_publisher_infos','hits_product_customDimensions','hits_product_customMetrics']\n",
    "    for col in bracket_col:\n",
    "        df[col] = df[col].str[0]\n",
    "    out_name = \"./data/out.{0}.{1}.csv\".format(flag,index)\n",
    "    df.to_csv(out_name,index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_input_csv(csv_path,chunksize=5000,flag=None):\n",
    "    time_beg = datetime.now()\n",
    "    dfs = pd.read_csv(csv_path, sep=',',\n",
    "            converters={column: json.loads for column in JSON_COLUMNS}, \n",
    "            dtype={'fullVisitorId': 'str'}, # Important!!\n",
    "            chunksize=chunksize)\n",
    "    jobs = []\n",
    "    for index,df in enumerate(dfs):\n",
    "        print(\"--> job {} started\".format(index))\n",
    "        p = multiprocessing.Process(target=work_on_one_reader,args=(df,index,flag,))\n",
    "        p.start()\n",
    "        jobs.append(p)\n",
    "    for index,job in enumerate(jobs):\n",
    "        job.join()\n",
    "    \n",
    "    # read in all processed csv file and concat together\n",
    "    path = \"./data\"\n",
    "    all_file = glob.glob(path+\"/out.{}.*.csv\".format(flag))\n",
    "    list_ = []\n",
    "    new_df = pd.DataFrame()\n",
    "    for file in all_file:\n",
    "        df = pd.read_csv(file,low_memory=False)\n",
    "        list_.append(df)\n",
    "    new_df = pd.concat(list_)\n",
    "    time_end = datetime.now()\n",
    "    print(\"----> Total time spent is {}\".format(time_end - time_beg))\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> job 0 started\n",
      "--> job 1 started\n",
      "--> job 2 started\n",
      "--> job 3 started\n",
      "--> job 4 started\n",
      "--> job 5 started\n",
      "--> job 6 started\n",
      "--> job 7 started\n",
      "--> job 8 started\n",
      "--> job 9 started\n",
      "--> job 10 started\n",
      "--> job 11 started\n",
      "--> job 12 started\n",
      "--> job 13 started\n",
      "--> job 14 started\n",
      "--> job 15 started\n",
      "--> job 16 started\n",
      "--> job 17 started\n",
      "--> job 18 started\n",
      "--> job 19 started\n",
      "--> job 20 started\n",
      "--> job 21 started\n",
      "--> job 22 started\n",
      "--> job 23 started\n",
      "--> job 24 started\n",
      "--> job 25 started\n",
      "--> job 26 started\n",
      "--> job 27 started\n",
      "--> job 28 started\n",
      "--> job 29 started\n",
      "--> job 30 started\n",
      "--> job 31 started\n",
      "--> job 32 started\n",
      "--> job 33 started\n",
      "--> job 34 started\n",
      "--> job 35 started\n",
      "--> job 36 started\n",
      "--> job 37 started\n",
      "--> job 38 started\n",
      "--> job 39 started\n",
      "--> job 40 started\n",
      "--> job 41 started\n",
      "--> job 42 started\n",
      "--> job 43 started\n",
      "--> job 44 started\n",
      "--> job 45 started\n",
      "--> job 46 started\n",
      "--> job 47 started\n",
      "--> job 48 started\n",
      "--> job 49 started\n",
      "--> job 50 started\n",
      "--> job 51 started\n",
      "--> job 52 started\n",
      "--> job 53 started\n",
      "--> job 54 started\n",
      "--> job 55 started\n",
      "--> job 56 started\n",
      "--> job 57 started\n",
      "--> job 58 started\n",
      "--> job 59 started\n",
      "--> job 60 started\n",
      "--> job 61 started\n",
      "--> job 62 started\n",
      "--> job 63 started\n",
      "--> job 64 started\n",
      "--> job 65 started\n",
      "--> job 66 started\n",
      "--> job 67 started\n",
      "--> job 68 started\n",
      "--> job 69 started\n",
      "--> job 70 started\n",
      "--> job 71 started\n",
      "--> job 72 started\n",
      "--> job 73 started\n",
      "--> job 74 started\n",
      "--> job 75 started\n",
      "--> job 76 started\n",
      "--> job 77 started\n",
      "--> job 78 started\n",
      "--> job 79 started\n",
      "--> job 80 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:24: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----> Total time spent is 0:14:04.575114\n"
     ]
    }
   ],
   "source": [
    "test_df = process_input_csv(\"./data/test_v2.csv\",chunksize=5000,flag=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('./data/my_test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = process_input_csv(\"./data/train_v2.csv\",chunksize=5000,flag=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_df.to_csv('./data/my_train.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files_to_remove = glob.glob(\"./data\"+\"/out.*csv\")\n",
    "for ff in all_files_to_remove:\n",
    "    os.remove(ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_anaconda3)",
   "language": "python",
   "name": "conda_anaconda3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
